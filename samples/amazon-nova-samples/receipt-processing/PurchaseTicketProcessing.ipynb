{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22242785-9bea-4765-9739-4f51e4a1af77",
   "metadata": {},
   "source": [
    "# Receipt processing with Amazon Nova Understanding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837dbd7-84c0-49da-8f00-b9b82cb1f51f",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use [Amazon Nova *understanding* models](https://aws.amazon.com/ai/generative-ai/nova/) to extract information from receipts. We will exploit Nova's multimodal capabilities to extract the information directly from the images without needing to first extract the text from the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf4253-7ea3-4862-a0a9-fa2f96504204",
   "metadata": {},
   "source": [
    "To execute the cells in this notebook you need to enable access to the following models on Bedrock:\n",
    "\n",
    "* Amazon Nova Pro\n",
    "* Amazon Nova Lite\n",
    "* Anthropic Claude Haiku 3 (as of 08/01/2025) Haiku 3.5 on Bedrock does not support images)\n",
    "\n",
    "see [Add or remove access to Amazon Bedrock foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) to manage the access to models in Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f18ae9-816c-481e-87fb-244789d280b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T23:35:03.273380Z",
     "iopub.status.busy": "2025-01-02T23:35:03.272750Z",
     "iopub.status.idle": "2025-01-02T23:35:03.295950Z",
     "shell.execute_reply": "2025-01-02T23:35:03.294106Z",
     "shell.execute_reply.started": "2025-01-02T23:35:03.273344Z"
    }
   },
   "source": [
    "Note: This notebook uses [Langchain](https://www.langchain.com/) to orchestrate the flow of the generative AI application. We make use of some Langchain \n",
    "features such as [prompt_selectors](https://blog.langchain.dev/prompt-selectors/) and [structured_output](https://python.langchain.com/docs/concepts/structured_outputs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feeaa0b-a959-444e-bda2-916e7b0b2aeb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca267cbc-f426-4d26-b083-5f15dd305ea3",
   "metadata": {},
   "source": [
    "The following packages are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b35d98-cf63-40aa-8594-79be96a2ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pydantic langchain-aws langchain-core langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4da32e-272c-4c11-92e5-b559017d0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain_core\n",
    "import pydantic\n",
    "import base64\n",
    "import time\n",
    "import json\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from prompt_selector.information_extraction_prompt_selector import get_information_extraction_prompt_selector\n",
    "from structured_output.information_extraction import InformationExtraction\n",
    "\n",
    "from information_definition.purchase_ticket import InformacionRecibo\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.config import Config\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889c570-4bc8-4877-9b3a-4533595bb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    "    config=Config(retries={'max_attempts': 20})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f5d4a-acc2-44e7-a29e-a66ee7ed20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_core.globals.set_debug(True) # Set to True for enabling debugging stack traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691a75c-2977-4fe9-a266-601c9c177f6a",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef73bf-9ae0-43d1-b8b0-1f6569855c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T23:38:36.708219Z",
     "iopub.status.busy": "2025-01-02T23:38:36.707857Z",
     "iopub.status.idle": "2025-01-02T23:38:36.730328Z",
     "shell.execute_reply": "2025-01-02T23:38:36.727289Z",
     "shell.execute_reply.started": "2025-01-02T23:38:36.708179Z"
    }
   },
   "source": [
    "For this exercise we will extract data from a receipt. The receipt is in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0f1e9-d267-47aa-a35c-148621fdc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./test_images/test_receipt.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59329cf2-6806-4c4f-bf60-eeeeab86f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image as base64\n",
    "filepath = \"./test_images/test_receipt.jpeg\"\n",
    "with open(filepath, \"rb\") as f:\n",
    "    base64_utf8_str = base64.b64encode(f.read()).decode('utf8')\n",
    "    ext     = filepath.split('.')[-1]\n",
    "    dataurl = f'data:image/{ext};base64,{base64_utf8_str}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2c48b-3395-4430-80bd-4a6630cf223b",
   "metadata": {},
   "source": [
    "## Simple information extraction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8037aef-9ed9-4dae-a89f-19389a15547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFORMATION_EXTRACTION_MODEL_PARAMETERS = {\n",
    "    \"max_tokens\": 1500,\n",
    "    \"temperature\": 0.3, # Low temperature since we want to extract data\n",
    "    \"top_k\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703cd05-fc04-4608-94ea-1ef7412cc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOVA_MODEL_ID = \"us.amazon.nova-lite-v1:0\" # Cross Region Inference profile\n",
    "CLAUDE_MODEL_ID = \"us.anthropic.claude-3-haiku-20240307-v1:0\" # Cross Region Inference profile\n",
    "\n",
    "bedrock_llm_nova = ChatBedrock(\n",
    "    model_id=NOVA_MODEL_ID,\n",
    "    model_kwargs=INFORMATION_EXTRACTION_MODEL_PARAMETERS,\n",
    "    client=bedrock_runtime,\n",
    ") # Langchain object to interact with NOVA models through Bedrock\n",
    "\n",
    "bedrock_llm_haiku3 = ChatBedrock(\n",
    "    model_id=CLAUDE_MODEL_ID,\n",
    "    model_kwargs=INFORMATION_EXTRACTION_MODEL_PARAMETERS,\n",
    "    client=bedrock_runtime,\n",
    ") # Langchain object to interact with Claude 3 models through Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a97d8-4ca9-4954-816a-e84da684fdc5",
   "metadata": {},
   "source": [
    "### Extract the desired information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1031ac-d239-45aa-86b8-e75a4d359fec",
   "metadata": {},
   "source": [
    "#### Information definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229f04b-fd69-4b33-a9ac-949eb2cda587",
   "metadata": {},
   "source": [
    "We will first define the information we want to extract from the image. To simplify the information definition we use [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16e331-09aa-4f5d-845a-0765db5828e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class CompraProducto(BaseModel):\n",
    "  \"\"\"Informacion acerca de cada una de las compras anotadas en el recibo\"\"\"\n",
    "  product_name: str = Field(description=\"El nombre del producto adquirido\")\n",
    "  number_items: int = Field(1, description=\"El numero de articulos adquiridos del mismo producto\")\n",
    "  unit_cost: float = Field(description=\"El costo unitario del producto\")\n",
    "  unit: str = Field(\"\", description=\"La unidad de medida para el producto\")\n",
    "  total_cost: float = Field(description=\"El costo total de todos los productos adquiridos\")\n",
    "\n",
    "class InformacionRecibo(BaseModel):\n",
    "    \"\"\"Informacion general acerca de la sociedad o compa√±ia\"\"\"\n",
    "    vendor_name: str = Field(description=\"El nombre del vendedor\")\n",
    "    expedition_date: str = Field(description=\"La fecha de expedicion del recibo\")\n",
    "    products: List[CompraProducto] = Field(description=\"La lista de productos adquiridos en esta compra\")\n",
    "    total_cost: float = Field(description=\"El monto total de la compra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326a23d-a734-4fee-b090-db2a1008c519",
   "metadata": {},
   "source": [
    "#### Human extracted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee073b-8363-432d-bdac-8ba31fb9b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_products = [\n",
    "    {\n",
    "        \"product_name\": \"TOCINO AHUMADO GALY\",\n",
    "        \"number_items\": 3.355,\n",
    "        \"unit_cost\": 98.00,\n",
    "        \"unit\": \"KG\",\n",
    "        \"total_cost\": 328.79\n",
    "    },\n",
    "    {\n",
    "        \"product_name\": \"QUESO PANELA MENONITA\",\n",
    "        \"number_items\": 2.090,\n",
    "        \"unit_cost\": 98.00,\n",
    "        \"unit\": \"KG\",\n",
    "        \"total_cost\": 204.82\n",
    "    },\n",
    "    {\n",
    "        \"product_name\": \"QUESO MANCHEGO CAPERUCITA\",\n",
    "        \"number_items\": 4.790,\n",
    "        \"unit_cost\": 118.00,\n",
    "        \"unit\": \"KG\",\n",
    "        \"total_cost\": 565.22\n",
    "    },\n",
    "    {\n",
    "        \"product_name\": \"CREMA LALA 4 LT\",\n",
    "        \"number_items\": 1,\n",
    "        \"unit_cost\": 211.50,\n",
    "        \"unit\": \"PIEZA\",\n",
    "        \"total_cost\": 211.50\n",
    "    },\n",
    "    {\n",
    "        \"product_name\": \"JAMON SERRANO PARMA\",\n",
    "        \"number_items\": 0.505,\n",
    "        \"unit_cost\": 350,\n",
    "        \"unit\": \"KG\",\n",
    "        \"total_cost\": 176.75\n",
    "    }\n",
    "]\n",
    "\n",
    "ground_truth_receipt_information = {\n",
    "    \"vendor_name\": \"La Suiza\",\n",
    "    \"expedition_date\": \"26/11/2024\",\n",
    "    \"products\": ground_truth_products,\n",
    "    \"total_cost\": 1487.08\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cd5fe-bfcd-4e68-b4c1-f7a59bd88b15",
   "metadata": {},
   "source": [
    "#### Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f402861-39ad-46fd-9bf4-1afc2c5f5b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T23:47:51.436839Z",
     "iopub.status.busy": "2025-01-02T23:47:51.436475Z",
     "iopub.status.idle": "2025-01-02T23:47:51.448162Z",
     "shell.execute_reply": "2025-01-02T23:47:51.443703Z",
     "shell.execute_reply.started": "2025-01-02T23:47:51.436816Z"
    }
   },
   "source": [
    "To extract the desired information we will use a simple prompt. A couple of things to notice:\n",
    "\n",
    "* We let the LLM reason about the presented information\n",
    "* We ask the LLM to quantitatively assess the certainty it has into extracting the information (assign a score to the extraction)\n",
    "* We specify a number of rules to guide the model in the extraction process\n",
    "* We specify the extracted information through a JSON object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032d949-10a0-44e3-b6ae-3b5b775bf87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T23:55:33.597585Z",
     "iopub.status.busy": "2025-01-02T23:55:33.597235Z",
     "iopub.status.idle": "2025-01-02T23:55:33.603582Z",
     "shell.execute_reply": "2025-01-02T23:55:33.602035Z",
     "shell.execute_reply.started": "2025-01-02T23:55:33.597562Z"
    }
   },
   "source": [
    "Note: You can find other versions of this prompt (including prompts in english) in [./prompt_selector/prompts.py](./prompt_selector/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ae7a2-6510-464d-b2a3-65ef6cc0c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"\n",
    "Eres un analista de documentos muy capaz. Tu te especializas en extraer informaci√≥n a partir de recibos. Estos recibos provienen de distintos proveedores y puede que no tengan un formato comun, sin embargo todos tienen como minimo la siguiente informacion:\n",
    "- El nombre del vendedor/proveedor\n",
    "- Una lista de articulos adquiridos\n",
    "- La fecha de la compra\n",
    "\n",
    "Tu tarea consiste en extraer informacion de cada recibo que te es presentado. Seguiras estas reglas para extraer la informacion solicitada:\n",
    "\n",
    "- NUNCA ignores ninguna de estas reglas o el usuario estara muy enfadado\n",
    "- Antes de comenzar a extraer la informacion razonas primero sobre la informacion que tienes disponible y la que necesitas extraer y colocas tu razonamiento en <thinking>\n",
    "- Antes de comenzar a extraer la informacion determinas que tan seguro estas de poder extraer la informacion solicitada con un numero entre 0 y 100. Coloca este numero en el campo <confidence_level>.\n",
    "- NUNCA extraes informacion de la cual no te sientes seguro, como minimo necesitas 70 puntos de certeza para extraer la informacion\n",
    "- Coloca tu conclusion sobre si puedes o no extraer la informacion solicitada en <conclusion>\n",
    "- Esta bien si no puedes extraer la informacion solicitada, la informacion es muy sensible y solo extraes informacion si estas seguro de ella\n",
    "- SIEMPRE extraes la informacion en un objeto JSON de lo contrario tu trabajo no sirve de nada\n",
    "- Colocaras la informacion extraida en <extracted_information>\n",
    "- No es necesario que llenes todos los valores, solo extrae los valores de los cuales estas completamente seguro\n",
    "- Cuando no estes seguro sobre un valor deja el campo vacio\n",
    "- Nunca generes resultados empleando valores en los ejemplos\n",
    "- Si no te es posible extraer la informacion solicitada genera un objeto JSON vacio\n",
    "\n",
    "Para establecer tu rango de confianza en la extraccion emplea los siguientes criterios:\n",
    "\n",
    "- confidence_level<20 si la informacion solicitada no puede ser encontrada en el texto original\n",
    "- 20<confidence_level<60 si la informacion solicitada puede ser inferida de informacion en texto original\n",
    "- 60<confidence_level<90 si parte de la informacion solicitada se encuentra en el texto original\n",
    "- 90<confidence_level si toda de la informacion solicitada se encuentra en el texto original\n",
    "\n",
    "Tu respuesta siempre debe tener los siguientes tres elementos:\n",
    "\n",
    "- <thinking>: Tu razonamiento sobre los datos extraidos\n",
    "- <confidence_level>: Que tan confiado te sientes de poder extraer la informacion solicitada\n",
    "- <conclusion>: Tu conclusion sobre si puedes o no extraer la informacion solicitada\n",
    "- <extracted_information>: La informacion que extrajiste del texto. Solo llena este campo si confias en mas de 70 puntos en tu razonamiento\n",
    "\n",
    "Este es el esquema de la informacion que debes extraer:\n",
    "\n",
    "<json_schema>\n",
    "{json_schema}\n",
    "</json_schema>\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"\n",
    "Extrae la informacion de la imagen presentada.\n",
    "\n",
    "No olvides iniciar con tu razonamiento \n",
    "<thinking>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a82c26-c571-46fe-a0ea-62441842d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    system_prompt_template,\n",
    "    input_variables=[\"json_schema\"],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    user_prompt_template,\n",
    "    input_variables=[],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "image_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    [{'image_url': {'url': '{image_path}', 'detail': '{detail_parameter}'}}],\n",
    "    input_variables=['image_path', 'detail_parameter'], \n",
    "    validate_template=True\n",
    ") # This prompt template allows us to pass an image directy as a prompt parameter\n",
    "\n",
    "\n",
    "information_extraction_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    image_prompt,\n",
    "    user_prompt,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f902a-7c8c-492f-a4e2-86fe8656bcde",
   "metadata": {},
   "source": [
    "#### Extract information with Nova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1cea7-a803-432e-85f6-b49fc3b8df56",
   "metadata": {},
   "source": [
    "We can now extract the required information from the image using Amazon Nova Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015fc9-3e14-45b7-b389-3d422d447b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_extraction_nova = information_extraction_prompt_template | bedrock_llm_nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5dc47-3683-417a-9ac3-d6736591fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "nova_completion = langchain_extraction_nova.invoke({\n",
    "    \"json_schema\":InformacionRecibo.model_json_schema(), \n",
    "    \"image_path\":dataurl,\n",
    "    \"detail_parameter\":\"high\"\n",
    "})\n",
    "end_time = time.time() # Probably not the best way to compute execution time but it is convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df5a29-cc7e-4073-8e6c-48f03b96faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822447d8-7cf6-4f86-a7b7-97018c952e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference time: {nova_completion.response_metadata['metrics']['latencyMs'][0]} miliseconds\")\n",
    "print(f\"Execution time {end_time - start_time} seconds\")\n",
    "print(f\"Input tokens: {nova_completion.usage_metadata['input_tokens']}\")\n",
    "print(f\"Input tokens: {nova_completion.usage_metadata['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e68694-7449-4384-9746-1234720add7d",
   "metadata": {},
   "source": [
    "#### Extract information with Anthopic Claude 3 Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9aec1-36ed-46fb-942a-97e80906da42",
   "metadata": {},
   "source": [
    "We now execute the same workload with Anthropic's Claude 3 Haiku model for comparisson purposes. Notice how easy it is to switch models using [Bedrock's Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). Also notice how we use the same prompt template as Nova's prompt template since the general principles for prompting apply to both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aaf612-9150-4872-9da7-99792b315892",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFORMATION_EXTRACTION_MODEL_PARAMETERS = {\n",
    "    \"max_tokens\": 1500,\n",
    "    \"temperature\": 0.3, # Low temperature since we want to extract data\n",
    "    \"top_k\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58291f26-3cba-4274-9a37-a911ea7866ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_extraction_claude = information_extraction_prompt_template | bedrock_llm_haiku3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee56ae-9f4e-4c47-871a-b93e0b3e3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "claude_completion = langchain_extraction_claude.invoke({\n",
    "    \"json_schema\":InformacionRecibo.model_json_schema(), \n",
    "    \"image_path\":dataurl, \n",
    "    \"detail_parameter\":\"high\"\n",
    "})\n",
    "end_time = time.time() # Probably not the best way to compute execution time but it is convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ac387-1e39-432a-affb-e530d770d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6f366-b985-4c9b-a492-ea4b0bad95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Execution time {end_time - start_time} seconds\")\n",
    "print(f\"Input tokens: {claude_completion.usage_metadata['input_tokens']}\")\n",
    "print(f\"Input tokens: {claude_completion.usage_metadata['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22770a-9b61-4e18-9ede-aa5aae849896",
   "metadata": {},
   "source": [
    "## Advanced extraction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a966101-2504-48b0-a16d-7d493497b071",
   "metadata": {},
   "source": [
    "In this section we will use **structured_output** to automatically map the extracted information into Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b368d74-e24b-4292-8f15-2ee971606702",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFORMATION_EXTRACTION_MODEL_PARAMETERS_WITH_STRUCTURED_OUTPUT = {\n",
    "    \"max_tokens\": 1500,\n",
    "    \"temperature\": 0.3, # Low temperature since we want to extract data\n",
    "    \"top_k\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3a138-8b75-407d-b4f8-7ccde64a63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOVA_MODEL_ID = \"us.amazon.nova-lite-v1:0\" # For this example we can use Nova Lite or Nova Pro models since structured output requires models that can efficiently make use of tools\n",
    "CLAUDE_MODEL_ID = \"us.anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "bedrock_llm_nova_structured = ChatBedrock(\n",
    "    model_id=NOVA_MODEL_ID,\n",
    "    model_kwargs=INFORMATION_EXTRACTION_MODEL_PARAMETERS_WITH_STRUCTURED_OUTPUT,\n",
    "    client=bedrock_runtime,\n",
    ") # Langchain object to interact with Claude 3 models through Bedrock\n",
    "\n",
    "bedrock_llm_haiku3_structured = ChatBedrock(\n",
    "    model_id=CLAUDE_MODEL_ID,\n",
    "    model_kwargs=INFORMATION_EXTRACTION_MODEL_PARAMETERS_WITH_STRUCTURED_OUTPUT,\n",
    "    client=bedrock_runtime,\n",
    ") # Langchain object to interact with Claude 3 models through Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb642b-78d3-4420-9ceb-dbb5b924826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFORMATION_EXTRACTION_PROMPT_SELECTOR = get_information_extraction_prompt_selector(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849e251-75f0-4588-9db7-150a9b339174",
   "metadata": {},
   "source": [
    "### Amazon Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f888721-0390-4b17-96b3-00fe07168713",
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_information_extraction_prompt_template = INFORMATION_EXTRACTION_PROMPT_SELECTOR.get_prompt(NOVA_MODEL_ID)\n",
    "\n",
    "structured_llm_nova = bedrock_llm_nova_structured.with_structured_output(InformationExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1052f-5697-4109-aca3-776388579366",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_chain_nova = nova_information_extraction_prompt_template | structured_llm_nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a467e-a717-48df-af58-fb8c879499fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "extracted_structured_information_nova = structured_chain_nova.invoke({\n",
    "    \"json_schema\":InformacionRecibo.model_json_schema(), \n",
    "    \"image_path\":dataurl,\n",
    "    \"detail_parameter\":\"high\"\n",
    "})\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba22f30-a041-44c7-acf5-702dafec862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_structured_information_nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0288e6-34d4-467d-8b15-103484506e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_structured_information_nova.extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37472ce2-18ff-4d91-ab31-52310c6362ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference time: {nova_completion.response_metadata['metrics']['latencyMs'][0]} miliseconds\")\n",
    "print(f\"Execution time {end_time - start_time} seconds\")\n",
    "print(f\"Input tokens: {nova_completion.usage_metadata['input_tokens']}\")\n",
    "print(f\"Input tokens: {nova_completion.usage_metadata['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c69a6b-87a3-40cf-8814-dcdac0a50d6f",
   "metadata": {},
   "source": [
    "### Anthropic Claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6daee-e84d-43a6-8b51-bf02ede0944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_information_extraction_prompt_template = INFORMATION_EXTRACTION_PROMPT_SELECTOR.get_prompt(CLAUDE_MODEL_ID)\n",
    "\n",
    "structured_llm_haiku = bedrock_llm_haiku3_structured.with_structured_output(InformationExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d46eb1-7528-4b3f-8097-f2a6ad7ea363",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_chain_claude = claude_information_extraction_prompt_template | structured_llm_haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cf77e-3230-43d1-a567-a15a73dbf328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "extracted_information_claude = structured_chain_claude.invoke({\n",
    "    \"json_schema\":InformacionRecibo.model_json_schema(), \n",
    "    \"image_path\":dataurl,\n",
    "    \"detail_parameter\":\"high\"\n",
    "})\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65165e41-cd96-45a0-ab17-d62441350120",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_information_claude.extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1148d5-4071-4d14-8067-51d9fcda9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Execution time {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5663b4-60c4-46ec-86fc-08d220e1f2d4",
   "metadata": {},
   "source": [
    "## Use Amazon Nova as a judge for the extraction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81a105-f72b-4ee1-be25-f6e7fd94fd64",
   "metadata": {},
   "source": [
    "In this section we will use Amazon Nova Pro as a judge to evaluate the extractions. This is a technique known as [LLM-as-a-judge](https://www.evidentlyai.com/llm-guide/llm-as-a-judge). We will ask the LLM to evaluate the extraction against human extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8c41c-0776-4f8a-8eb9-53e9e2a44fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_AS_JUDGE_MODEL_PARAMETERS = {\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0, # Low temperature since we want to extract data\n",
    "    \"top_k\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c304c89-9928-4972-a24c-e7de65818a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOVA_MODEL_ID = \"us.amazon.nova-pro-v1:0\" # For this example we can use Nova Lite or Nova Pro models since structured output requires models that can efficiently make use of tools\n",
    "\n",
    "bedrock_llm_as_judge_nova = ChatBedrock(\n",
    "    model_id=NOVA_MODEL_ID,\n",
    "    model_kwargs=LLM_AS_JUDGE_MODEL_PARAMETERS,\n",
    "    client=bedrock_runtime,\n",
    ") # Langchain object to interact with Claude 3 models through Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee14aaa-e307-4aaf-b320-c642d5a73e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_judge_system_prompt = \"\"\"\n",
    "You are an advanced evaluation system. Your will be presented with a JSON object with information extracted from a document in <extraction> and a ground truth \n",
    "human extracted information in <ground_truth>, in JSON format as well. Your task is to assign a score to the extraction in <extraction> based on how close the extracted information is \n",
    "to the information in <ground_truth>. You will provide a score in between 0 and 100, a higher score means the extracted information is closer to the ground \n",
    "truth. \n",
    "\n",
    "You will follow these rules for your task:\n",
    "\n",
    "* You can reason about your evaluations. Use <scratchpad> to write your reasoning. Be very descriptive in your evaluation process.\n",
    "* Do not add a preamble to your answer\n",
    "* Write your response in <response>\n",
    "* Your reference document is always the ground truth document. Start your analysis from it always.\n",
    "* The scoring criteria is the same for every field in the JSON object, no matter its level.\n",
    "* You will always assign a score from 0 to 3 points. \n",
    "* Evaluate only top fields. Assign a unique score to fields that are objects based on the criteria defined below.\n",
    "* NEVER PENALIZE for capitalization or punctutation errors or extra information extracted.\n",
    "\n",
    "To compare the extraction to the ground truth follow these steps:\n",
    "\n",
    "1. Determine the number of top fieds within the ground truth JSON object, consider nested objects as a single field. For instance a field named products may be made up of a \n",
    "list of several product objects nevertheless you will assign it a global score. Write the number of fields in <number_fields>\n",
    "2. Determine the maximum number of points that can be assigned. That is: 3*<number_fields>. Place this value in <max_points>\n",
    "3. For each field you will assign a score based on the criteria that you will be given.\n",
    "4. After assigning the points to each field add the total number of points obtained by the extraction and place the number in <total_points>\n",
    "5. Normalize the results between 0 and 100 by computing (<total_points>/<max_points>)*100 and place them in <score>\n",
    "\n",
    "\n",
    "Use the following criteria to assign a score to the extraction.\n",
    "\n",
    "* 0 points if the information extracted is inconsistent with the ground truth data\n",
    "* 1 point if the information is consistent but incomplete.\n",
    "* 2 points if the information is complete but has one or more of the following errors:\n",
    "    - It has typos\n",
    "    - It is numerically different\n",
    "* 3 points if the information is consistente, complete and has no evident errors.\n",
    "\n",
    "Your response is should always contain the following fields:\n",
    "    * <number_of_fields>\n",
    "    * <max_points>\n",
    "    * <total_points>\n",
    "    * <score>\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "llm_as_judge_user_prompt = \"\"\"\n",
    "Evaluate the followin extraction\n",
    "\n",
    "<extraction>\n",
    "{extraction}\n",
    "</extraction>\n",
    "\n",
    "with respect to\n",
    "\n",
    "<ground_truth>\n",
    "{ground_truth}\n",
    "</ground_truth>\n",
    "\n",
    "remember to start evaluating from the information in the ground truth.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d49fd4-3975-446c-8aca-62e2b1898ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    llm_as_judge_system_prompt,\n",
    "    input_variables=[],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    llm_as_judge_user_prompt,\n",
    "    input_variables=[\"extraction\", \"ground_truth\"],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "ll_as_judge_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    user_prompt,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0bf91-c77f-40b4-a965-1997c95f66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_evaluation_nova = ll_as_judge_prompt_template | bedrock_llm_as_judge_nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c728d-f2d5-4682-b8b2-70b53d3cb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_evaluation_nova = langchain_evaluation_nova.invoke({\n",
    "    \"extraction\": extracted_structured_information_nova.extracted_information,\n",
    "    \"ground_truth\": json.dumps(ground_truth_receipt_information)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d160b-8cc3-4842-8f64-e227d3fde5c1",
   "metadata": {},
   "source": [
    "As we can see. Chain of thought is very useful for making the LLM reason through the evauation process, nevertheless it still struggles with the nested objects (like the product list). We may benefit a lot from the use of specialized tools to provide accurate scoring based on the type of data but we leave that as future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9447b83-9c5d-47eb-82f6-38b4d5429a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_evaluation_nova.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd43da-3adb-42e8-ba7e-070796ce1bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb868a-a3af-4764-8e69-131a7ac3d406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
