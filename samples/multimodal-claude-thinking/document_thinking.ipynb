{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from claude_think import ClaudeThink\n",
    "ct = ClaudeThink()\n",
    "\n",
    "def download_file(url: str, save_path: str = None) -> None:\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        content = response.content\n",
    "        \n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "\n",
    "        return response.content        \n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return b''\n",
    "\n",
    "\n",
    "def read_binary_file(file_path: str) -> bytes:\n",
    "    try:\n",
    "        \n",
    "        with open(file_path, 'rb') as binary_file:\n",
    "            content = binary_file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Binary file not found at path: {file_path}\")\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"Error reading binary file: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_bytes = download_file(\"https://arxiv.org/pdf/2501.12948\",\"2501.12948.pdf\" )\n",
    "file_bytes = read_binary_file(\"2501.12948.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "    {\"text\": \"Can you explain this paper in common language? why is a breaktrough?\"},\n",
    "    { \n",
    "        \"document\": { \n",
    "            \"name\": \"250112948\", \"format\": \"pdf\", \"source\": { \"bytes\": file_bytes}\n",
    "        }}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Thinking...***\n",
       "\n",
       " <em>I need to explain the DeepSeek-R1 paper in common language and identify why it's considered a breakthrough. Let me analyze the key aspects of the paper:\n",
       "\n",
       "Key Aspects of the Paper:\n",
       "1. Introduction of DeepSeek-R1-Zero and DeepSeek-R1 - a new generation of reasoning models\n",
       "2. Training approach using Reinforcement Learning (RL) without Supervised Fine-Tuning (SFT)\n",
       "3. Competitive performance with OpenAI's o1 models\n",
       "4. Distillation of reasoning capabilities to smaller models\n",
       "5. Breakthrough elements and novel contributions\n",
       "\n",
       "Let me organize this into a clear explanation focusing on:\n",
       "1. What the paper is about\n",
       "2. The key innovation (using pure RL for reasoning)\n",
       "3. Why this is considered a breakthrough\n",
       "4. The results and implications\n",
       "5. The importance of the distillation findings</em>\n",
       "\n",
       "***Final Answer:***\n",
       "\n",
       " # DeepSeek-R1: A Breakthrough in AI Reasoning Through Reinforcement Learning\n",
       "\n",
       "## What's This Paper About?\n",
       "\n",
       "This paper introduces DeepSeek-R1, a new AI model designed specifically to excel at reasoning tasks like mathematics, coding, and scientific problems. What makes this work special is *how* they trained the model to reason.\n",
       "\n",
       "## The Key Innovation\n",
       "\n",
       "The researchers discovered that they could teach an AI to reason better without using the standard industry approach. Typically, AI models are first trained on supervised examples (showing them exactly how to solve problems step-by-step), but DeepSeek found they could skip this step entirely.\n",
       "\n",
       "Instead, they used reinforcement learning (RL) - essentially giving the model rewards when it gets answers right, without explicitly teaching it how to solve problems. This is like teaching a child to solve math by only telling them when their final answer is correct, rather than showing them the solution method.\n",
       "\n",
       "## Why It's a Breakthrough\n",
       "\n",
       "This approach represents a breakthrough for several reasons:\n",
       "\n",
       "1. **Self-learning capability**: The model (called DeepSeek-R1-Zero) naturally developed sophisticated reasoning behaviors on its own, including:\n",
       "   - Breaking problems into smaller steps\n",
       "   - Verifying its own work\n",
       "   - Reflecting on earlier steps when stuck\n",
       "   - Generating extensive reasoning chains\n",
       "\n",
       "2. **Performance parity with closed systems**: Their model achieves results comparable to OpenAI's o1 models (some of the best reasoning models available), but with a completely different training approach.\n",
       "\n",
       "3. **\"Aha moments\"**: The model spontaneously learned to spend more time thinking about complex problems and developed human-like behaviors like saying \"Wait, let me reconsider this\" when it realizes it's on the wrong track.\n",
       "\n",
       "4. **Open source contribution**: They're sharing their models with the research community, which is significant as most top-performing reasoning models are closed systems.\n",
       "\n",
       "## The Results\n",
       "\n",
       "DeepSeek-R1 performs impressively on challenging benchmarks:\n",
       "- 79.8% accuracy on AIME 2024 (advanced math competition problems)\n",
       "- 97.3% on MATH-500 (complex math problems)\n",
       "- Outstanding performance on coding challenges (better than 96.3% of human competitors)\n",
       "\n",
       "## Distillation Discovery\n",
       "\n",
       "Perhaps equally important, they discovered that reasoning capabilities can be \"distilled\" from larger models to much smaller ones. This means even small models (7B parameters) can outperform much larger general models (like GPT-4o) on reasoning tasks after being trained on outputs from the specialized reasoning model.\n",
       "\n",
       "This is significant because it means powerful reasoning capabilities can be deployed on devices with limited resources, making advanced AI reasoning more accessible and practical.\n",
       "\n",
       "The breakthrough nature of this work isn't just the results, but the demonstration that AI systems can develop sophisticated reasoning strategies through simple reward signals, without explicit teaching of reasoning methods. This suggests a more human-like learning approach might be possible in AI development. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning, answer = ct.converse_stream(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Thinking...***\n",
       "\n",
       " <em>Figure 3 in the paper shows a graph titled \"The average response length of DeepSeek-R1-Zero on the training set during the RL process.\" The figure demonstrates how the average response length of DeepSeek-R1-Zero evolves throughout the reinforcement learning (RL) training process.\n",
       "\n",
       "The x-axis appears to represent the training steps or iterations, while the y-axis shows the average response length (likely measured in tokens).\n",
       "\n",
       "From the graph, we can see a clear upward trend, indicating that as training progresses, the model naturally learns to generate longer responses. The caption below the figure states: \"DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\"\n",
       "\n",
       "This is significant because:\n",
       "\n",
       "1. The model wasn't explicitly instructed to produce longer responses\n",
       "2. It naturally developed the behavior of using more tokens (more \"thinking time\") to solve complex reasoning problems\n",
       "3. This emergent behavior happened organically through the reinforcement learning process\n",
       "\n",
       "The graph illustrates one of the key points in the paper: through pure reinforcement learning, without any supervised fine-tuning or explicit instructions to reason step-by-step, the model discovers on its own that spending more time thinking (generating more tokens) helps it solve reasoning tasks more effectively.\n",
       "\n",
       "This is part of what the authors refer to as the \"self-evolution process\" of DeepSeek-R1-Zero, where the model autonomously develops more sophisticated reasoning capabilities as training progresses.</em>\n",
       "\n",
       "***Final Answer:***\n",
       "\n",
       " # Explaining Figure 3: The Evolution of AI \"Thinking Time\"\n",
       "\n",
       "Figure 3 in the paper shows a fascinating phenomenon that occurred during the training of DeepSeek-R1-Zero. The graph displays the **average response length** (essentially how much text the AI generates when solving problems) during the reinforcement learning process.\n",
       "\n",
       "## What's Happening in the Graph:\n",
       "\n",
       "1. The x-axis represents the progression of training (training steps)\n",
       "2. The y-axis shows how many tokens (roughly words/characters) the AI uses in its responses\n",
       "3. There's a clear upward trend as training progresses\n",
       "\n",
       "## The Significance:\n",
       "\n",
       "This graph captures something remarkable: **the AI naturally learned to \"think longer\"** to solve problems. Without being explicitly programmed to do so, the model discovered on its own that generating more extensive reasoning chains helped it arrive at correct answers.\n",
       "\n",
       "Think of it like this: initially, the AI might try to solve a complex math problem in just a few steps. But as training progresses, it learns through trial and error that taking more time to work through multiple steps, check its work, and explore different approaches leads to better results.\n",
       "\n",
       "This is particularly noteworthy because:\n",
       "\n",
       "- The researchers never told the model \"use more tokens\" or \"think longer\"\n",
       "- This behavior emerged organically through simple reward signals (correct/incorrect answers)\n",
       "- It mirrors how humans approach difficult problems - we typically spend more time thinking through complex issues\n",
       "\n",
       "The authors highlight this as evidence of the model's \"self-evolution\" - discovering effective problem-solving strategies on its own rather than having them programmed in or learned from human demonstrations. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning, answer = ct.converse_stream([{\n",
    "                \"text\": \"What is going on in figure 3?, please explain\"\n",
    "            }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
