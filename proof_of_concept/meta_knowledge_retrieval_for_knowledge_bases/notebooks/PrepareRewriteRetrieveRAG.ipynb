{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare-Rewrite-Retrieve RAG flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the data retrieval and generation process of the **prepare-then-rewrite-then-retrieve-then-read** framework proposed by the authors of [\"Meta Knowledge for Retrieval Augmented Large Language Models\"](https://www.amazon.science/publications/meta-knowledge-for-retrieval-augmented-large-language-models) for creating more accurate and enriched RAG workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "To run this notebook your role executing the notebook needs:\n",
    "\n",
    "* Permissions to invoke Bedrock\n",
    "* Access to the Amazon Nova Pro model\n",
    "* Having executed the [DataIndexing.ipynb](./DataIndexing.ipynb) notebook\n",
    "\n",
    "Additionally, we need the following python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U boto3 langchain langchain-aws dotenv faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import secrets\n",
    "import time\n",
    "import boto3\n",
    "import faiss\n",
    "import langchain_core\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from enum import Enum\n",
    "from PyPDF2 import PdfReader\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from prompts.dataRetrieval.generate_query_augmentation_prompts import get_query_augmentation_prompt_selector, get_structured_questions_prompt_selector\n",
    "from prompts.dataRetrieval.generate_qa_kb_prompts import get_kb_qa_prompt_selector\n",
    "from structured_output.questions import Questions\n",
    "from structured_output.answers import Answer\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "langchain_core.globals.set_debug(False)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "EMBEDDINGS_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "EMBEDDING_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-KB-Summaries and VectorStore\n",
    "\n",
    "This knowledge base is made up of two components:\n",
    "\n",
    "* A vector store: As in any other knowledge base the main component is a vector store where the embeddings are persisted. For this implementation we use [FAISS](https://faiss.ai/index.html) as the vector store.\n",
    "* A meta-knowledge base: Consisting of the summaries of each partition of the knowledge base. For this implementation the meta-knowledge base is stored in a simple python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a python dictionary as meta-knowledge summary\n",
    "with open(\"./data/meta_kb_summaries.json\", \"r\") as f:\n",
    "    meta_kb_summaries = json.load(f)\n",
    "          \n",
    "\n",
    "# We use a FAISS vectorstore as knowledge base\n",
    "embeddings_model = BedrockEmbeddings(\n",
    "    model_id=EMBEDDINGS_MODEL_ID,\n",
    "    model_kwargs={\"dimensions\": EMBEDDING_SIZE},\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "vector_index = faiss.IndexFlatL2(EMBEDDING_SIZE)\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"./data/faiss_index\", \n",
    "    embeddings=embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize according to the types of document to be processed by the application\n",
    "class DocumentTypes(Enum):\n",
    "    SYSTEM_ARCHITECTURE = \"systems architecture\"\n",
    "    SECURITY = \"information technology security\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    TECH_STRATEGY = \"tech strategy\"\n",
    "    MANAGEMENT = \"management\"\n",
    "\n",
    "class AnalysisPerspectives(Enum):\n",
    "    SECURITY = \"software security engineer\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    RESILIENCY = \"systems resiliency\"\n",
    "    SYS_OPS = \"systems operations\"\n",
    "\n",
    "# Persona definition for generating and answering QA\n",
    "AnalysisPersonas = {\n",
    "    \"software security engineer\": {\n",
    "        \"description\": \"It is responsible for ensuring that workloads have the necessary security controls in place\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SECURITY.value, AnalysisPerspectives.DATA_GOVERNANCE.value]\n",
    "    },\n",
    "    \"solutions architect\": {\n",
    "        \"description\": \"It is responsible for designing scalable and cost-efficient software solutions\",\n",
    "        \"perspectives\": [AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.DATA_GOVERNANCE.value, AnalysisPerspectives.SECURITY.value]\n",
    "    },\n",
    "    \"software developer\": {\n",
    "        \"description\":\"Implements the system functionalities\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SYS_OPS.value, AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.SECURITY.value]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_PERSONNA = \"solutions architect\"\n",
    "ANALYSIS_PERSPECTIVE = AnalysisPersonas[ANALYSIS_PERSONNA][\"perspectives\"][0]\n",
    "\n",
    "print(f\"Using persona: {ANALYSIS_PERSONNA}\")\n",
    "print(f\"Using perspective: {ANALYSIS_PERSPECTIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_chatbot_answer(\n",
    "    user_query,\n",
    "    role,\n",
    "    perspective,\n",
    "    context\n",
    "):\n",
    "    \"Answer query given context using LLMs\"\n",
    "\n",
    "    print(user_query)\n",
    "\n",
    "    rag_llm = ChatBedrockConverse(\n",
    "        model=BEDROCK_MODEL_ID,\n",
    "        temperature=0.4,\n",
    "        max_tokens=1000,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    LLM_KB_QA_PROMPT_SELECTOR = get_kb_qa_prompt_selector(lang=\"en\")\n",
    "    \n",
    "    gen_kb_qa_prompt = LLM_KB_QA_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "    \n",
    "    kb_qa_generate = gen_kb_qa_prompt | rag_llm.with_structured_output(Answer)\n",
    "\n",
    "    rag_qa = kb_qa_generate.invoke(\n",
    "        {\n",
    "            \"question\": user_query,\n",
    "            \"role\": role,\n",
    "            \"perspective\": perspective,\n",
    "            \"context\": context\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return rag_qa\n",
    "\n",
    "def augment_user_query(\n",
    "        role,\n",
    "        user_query,\n",
    "        mk_summary,\n",
    "):\n",
    "    \"Augment the user query with additional queries based on the meta-knowledge summary.\"\n",
    "    \n",
    "    query_augmentation_llm = ChatBedrockConverse(\n",
    "        model=BEDROCK_MODEL_ID,\n",
    "        temperature=0.4,\n",
    "        max_tokens=2000,\n",
    "        # other params...\n",
    "    )\n",
    "    \n",
    "    LLM_AUGMENT_QUERY_PROMPT_SELECTOR = get_query_augmentation_prompt_selector(lang=\"en\")\n",
    "    \n",
    "    gen_queries_prompt = LLM_AUGMENT_QUERY_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "    structured_queries = query_augmentation_llm.with_structured_output(Questions)\n",
    "    \n",
    "    structured_queries_generate = gen_queries_prompt | structured_queries\n",
    "\n",
    "    augmented_queries = structured_queries_generate.invoke(\n",
    "        {\n",
    "            \"role\": role,\n",
    "            \"mk_summary\": mk_summary,\n",
    "            \"user_query\": user_query\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return augmented_queries\n",
    "\n",
    "\n",
    "def prepare_rewrite_retrieve_rag_qa(\n",
    "    query,\n",
    "    persona,\n",
    "    perspective\n",
    "):\n",
    "    \"\"\"Answer a user query using the prepare-rewrite-retrieve framework from a persona-perspective point of view\"\"\"\n",
    "\n",
    "    qa_str = \"\"\n",
    "    qa_pairs = []\n",
    "\n",
    "    print(query)\n",
    "\n",
    "    #Augment user query\n",
    "    augmented_queries = augment_user_query(\n",
    "        role=persona,\n",
    "        user_query=query,\n",
    "        mk_summary=meta_kb_summaries[f\"{persona}-{perspective}\"]\n",
    "    )\n",
    "\n",
    "    print(augmented_queries)\n",
    "\n",
    "    # Retrieve context from knowledge base using metadata as partition keys\n",
    "    for question in augmented_queries.questions:\n",
    "        \n",
    "        results = vector_store.similarity_search(\n",
    "            query=question,\n",
    "            k=5,\n",
    "            filter={\"persona\": ANALYSIS_PERSONNA, \"perspective\": ANALYSIS_PERSPECTIVE}\n",
    "        )\n",
    "        retrieved_qa_pairs = [(result.metadata[\"question\"], result.metadata[\"answer\"]) for result in results]\n",
    "\n",
    "        qa_pairs.extend(retrieved_qa_pairs)\n",
    "\n",
    "    qa_str = qa_str.join(f\"Question:{qa[0]}\\nAnswer:{qa[1]}\\n\\n\" for qa in qa_pairs)\n",
    "\n",
    "    print(qa_str)\n",
    "\n",
    "    # Answer query using LLM\n",
    "    answer = qa_chatbot_answer(\n",
    "        user_query=query,\n",
    "        role=ANALYSIS_PERSONNA,\n",
    "        perspective=ANALYSIS_PERSPECTIVE,\n",
    "        context=qa_str\n",
    "    )\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering workflow with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is the purpose of the multi-agent compliance analysis project?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the original query is augmented using the meta-knowledge summary for the persona-perspective combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_kb_summary = meta_kb_summaries[f\"{ANALYSIS_PERSONNA}-{ANALYSIS_PERSPECTIVE}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_queries = augment_user_query(\n",
    "    role=ANALYSIS_PERSONNA,\n",
    "    user_query=QUERY,\n",
    "    mk_summary=meta_kb_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmenting the query:\")\n",
    "print(QUERY)\n",
    "\n",
    "print(\"\\n\\nExisting summary:\")\n",
    "print(meta_kb_summary)\n",
    "\n",
    "print(\"\\n\\nResulting queries:\")\n",
    "for question in augmented_queries.questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the augmented queries and retrieve information from the knowledge base using the augmented queries rather than the original query wich we will pass onto the LLM as context for the question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_str = \"\"\n",
    "qa_pairs = []\n",
    "\n",
    "# Retrieve context from knowledge base using metadata as partition keys\n",
    "for question in augmented_queries.questions:\n",
    "    \n",
    "    results = vector_store.similarity_search(\n",
    "        query=question,\n",
    "        k=5,\n",
    "        filter={\"persona\": ANALYSIS_PERSONNA, \"perspective\": ANALYSIS_PERSPECTIVE}\n",
    "    )\n",
    "    retrieved_qa_pairs = [(result.metadata[\"question\"], result.metadata[\"answer\"]) for result in results]\n",
    "\n",
    "    qa_pairs.extend(retrieved_qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can observe that we obtain indeed more comprehensive information but more fine-grained thanks to the indexing of Q&A pairs rather than chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa_pair in qa_pairs:\n",
    "    print(f\"Question: {qa_pair[0]}\")\n",
    "    print(f\"Answer: {qa_pair[1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering the original query with the context from augmented queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the retrieved information from the augmented queries as context to answer the original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_str = qa_str.join(f\"Question:{qa[0]}\\nAnswer:{qa[1]}\\n\\n\" for qa in qa_pairs)\n",
    "\n",
    "answer = qa_chatbot_answer(\n",
    "    user_query=QUERY,\n",
    "    role=ANALYSIS_PERSONNA,\n",
    "    perspective=ANALYSIS_PERSPECTIVE,\n",
    "    context=qa_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QUERY)\n",
    "print(qa_str)\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the purpose of the multi-agent compliance analysis project?\",\n",
    "    \"What security measures are in place to protect the data?\",\n",
    "    \"How is the multiagent orchestration done?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = prepare_rewrite_retrieve_rag_qa(\n",
    "    questions[0],\n",
    "    ANALYSIS_PERSONNA,\n",
    "    ANALYSIS_PERSPECTIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Q:{questions[0]}\")\n",
    "print(f\"A:{answer.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
