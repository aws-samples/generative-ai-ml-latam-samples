{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37c6aab",
   "metadata": {},
   "source": [
    "# Prepare-Rewrite-Retrieve knowledge base data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a7078",
   "metadata": {},
   "source": [
    "In this notebook we will explore the data ingestion process of the **prepare-then-rewrite-then-retrieve-then-read** framework proposed by the authors of [\"Meta Knowledge for Retrieval Augmented Large Language Models\"](https://www.amazon.science/publications/meta-knowledge-for-retrieval-augmented-large-language-models) for creating more accurate and enriched RAG workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb28b1",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "To run this notebook your role executing the notebook needs:\n",
    "\n",
    "* Permissions to invoke Bedrock\n",
    "* Access to the Amazon Nova Pro model\n",
    "* Write permissions to the DynamoDB table created with the CDK stack in this PoC\n",
    "* Write permissions to the OpenSearch Serverless host created with the CDK stack in this PoC\n",
    "\n",
    "Additionally, we need the following python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089be4a-ff4b-462b-ab99-66b2852d5e95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T04:13:48.442836Z",
     "iopub.status.busy": "2025-06-23T04:13:48.442358Z",
     "iopub.status.idle": "2025-06-23T04:13:57.004105Z",
     "shell.execute_reply": "2025-06-23T04:13:57.003138Z",
     "shell.execute_reply.started": "2025-06-23T04:13:48.442804Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 langchain langchain-community langchain-aws opensearch-py PyPDF2 dotenv faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4441559-02c2-4a19-aecd-5ddea7731b3a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-23T04:13:57.007534Z",
     "iopub.status.busy": "2025-06-23T04:13:57.007227Z",
     "iopub.status.idle": "2025-06-23T04:13:57.839476Z",
     "shell.execute_reply": "2025-06-23T04:13:57.838463Z",
     "shell.execute_reply.started": "2025-06-23T04:13:57.007509Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import secrets\n",
    "import time\n",
    "import boto3\n",
    "import faiss\n",
    "import langchain_core\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from enum import Enum\n",
    "from PyPDF2 import PdfReader\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from prompts.dataIngestion.generate_metadata_prompt import get_metadata_prompt_selector\n",
    "from prompts.dataIngestion.generate_qa_prompt import get_qa_prompt_selector, get_structured_qa_prompt_selector\n",
    "from prompts.dataIngestion.generate_meta_kb_prompt import get_summary_prompt_selector\n",
    "from structured_output.metadata import DocumentMetadata\n",
    "from structured_output.question_answers import QA_pairs\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356f866-b666-46ac-b3c8-bf77e4e93e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T04:13:57.844542Z",
     "iopub.status.busy": "2025-06-23T04:13:57.844140Z",
     "iopub.status.idle": "2025-06-23T04:13:57.848835Z",
     "shell.execute_reply": "2025-06-23T04:13:57.847408Z",
     "shell.execute_reply.started": "2025-06-23T04:13:57.844522Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "langchain_core.globals.set_debug(False)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b23f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "EMBEDDINGS_MODEL_ID=\"amazon.titan-embed-text-v2:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8c9dc",
   "metadata": {},
   "source": [
    "### Type definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09c1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize according to the types of document to be processed by the application\n",
    "class DocumentTypes(Enum):\n",
    "    SYSTEM_ARCHITECTURE = \"systems architecture\"\n",
    "    SECURITY = \"information technology security\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    TECH_STRATEGY = \"tech strategy\"\n",
    "    MANAGEMENT = \"management\"\n",
    "\n",
    "class AnalysisPerspectives(Enum):\n",
    "    SECURITY = \"software security engineer\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    RESILIENCY = \"systems resiliency\"\n",
    "    SYS_OPS = \"systems operations\"\n",
    "\n",
    "# Persona definition for generating and answering QA\n",
    "AnalysisPersonas = {\n",
    "    \"software security engineer\": {\n",
    "        \"description\": \"It is responsible for ensuring that workloads have the necessary security controls in place\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SECURITY.value, AnalysisPerspectives.DATA_GOVERNANCE.value]\n",
    "    },\n",
    "    \"solutions architect\": {\n",
    "        \"description\": \"It is responsible for designing scalable and cost-efficient software solutions\",\n",
    "        \"perspectives\": [AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.DATA_GOVERNANCE.value, AnalysisPerspectives.SECURITY.value]\n",
    "    },\n",
    "    \"software developer\": {\n",
    "        \"description\":\"Implements the system functionalities\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SYS_OPS.value, AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.SECURITY.value]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cac64",
   "metadata": {},
   "source": [
    "### AWS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d97922",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSEARCH_HOST = <OPENSEARCH_HOST> #URL (without the protocol) of the OpenSearch Serverless Host\n",
    "OPENSEARCH_PORT = 443 #Port of the OpenSearch Serverless Host\n",
    "\n",
    "OPERNSEARCH_INDEX_NAME = <OPENSEARCH_INDEX_NAME> #Name of the OpenSearch Serverless Index\n",
    "\n",
    "METAKB_DYNAMODB_TABLE_NAME = <METAKB_DYNAMODB_TABLE_NAME> #Name of the DynamoDB table created with the CDK stack in this PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c88d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"./docs/overallReadme.pdf\"\n",
    "DOCUMENT_KEY = \"overallReadme\"\n",
    "DOCUMENT_TITLE = \"Overall readme\"\n",
    "\n",
    "ANALYSIS_PERSONNA = \"solutions architect\"\n",
    "ANALYSIS_PERSPECTIVE = AnalysisPersonas[ANALYSIS_PERSONNA][\"perspectives\"][0]\n",
    "\n",
    "print(f\"Using persona: {ANALYSIS_PERSONNA}\")\n",
    "print(f\"Using perspective: {ANALYSIS_PERSPECTIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b5fda",
   "metadata": {},
   "source": [
    "## Create clients for AWS services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051963d6",
   "metadata": {},
   "source": [
    "This knowledge base is made up of two components:\n",
    "\n",
    "* A vector store: As in any other knowledge base the main component is a vector store where the embeddings are persisted. For this implementation we use [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) as vector store.\n",
    "* A meta-knowledge base: Consisting of the summaries of each partition of the knowledge base. For this implementation the meta-knowledge base is stored in an [Amazon DynamoDB](https://aws.amazon.com/dynamodb/) table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7c4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "botoSession = boto3.Session()\n",
    "\n",
    "meta_kb_table = botoSession.resource(\"dynamodb\").Table(METAKB_DYNAMODB_TABLE_NAME)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb64db",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576d1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(\n",
    "    pdf_path\n",
    "):\n",
    "    \"Extract text from a PDF file\"\n",
    "\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\" # Add newline for readability between pages\n",
    "    return text\n",
    "\n",
    "def encode_text(\n",
    "        text: str = None,  # the text to be encoded\n",
    "        dimension: int = 1024,  # 1,024 (default), 384, 256\n",
    "):\n",
    "    \"Get text embedding using embeddings model\"\n",
    "\n",
    "    payload_body = {\n",
    "        \"inputText\": text,\n",
    "        \"dimensions\": dimension,\n",
    "        \"normalize\": True\n",
    "    }\n",
    "\n",
    "    #print(\"embedding text\")\n",
    "    #print(payload_body)\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload_body),\n",
    "        modelId=EMBEDDINGS_MODEL_ID,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    feature_vector = json.loads(response.get(\"body\").read())[\"embedding\"]\n",
    "\n",
    "    #print(\"text embedding\")\n",
    "    #print(feature_vector)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def index_document(\n",
    "        oss_client,\n",
    "        oss_index_name,\n",
    "        embedding,\n",
    "        persona,\n",
    "        perspective,\n",
    "        question,\n",
    "        answer\n",
    "):\n",
    "    \"Index a document into Amazon Open Search Serverless\"\n",
    "\n",
    "    document = {\n",
    "        \"persona\": persona,\n",
    "        \"perspective\": perspective,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "    #print(document)\n",
    "\n",
    "    oss_response = oss_client.index(\n",
    "        index=oss_index_name,\n",
    "        body=document,\n",
    "    )\n",
    "\n",
    "    return oss_response\n",
    "\n",
    "\n",
    "def get_opensearch_connection(\n",
    "        host: str,\n",
    "        port: int,\n",
    ") -> OpenSearch:\n",
    "    \"Establishes a connection to an OpenSearch cluster using AWSV4SignerAuth for authentication.\"\n",
    "\n",
    "    # Create an AWSV4SignerAuth instance for authentication\n",
    "    auth = AWSV4SignerAuth(\n",
    "        boto3.Session(\n",
    "            region_name=os.getenv(\"AWS_REGION\")\n",
    "        ).get_credentials(),\n",
    "        os.getenv(\"AWS_REGION\"),\n",
    "        \"aoss\"\n",
    "    )\n",
    "\n",
    "    # Create an OpenSearch client instance\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": port}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=30,\n",
    "    )\n",
    "\n",
    "    # Return the OpenSearch client instance\n",
    "    return client\n",
    "\n",
    "def get_existing_summary(\n",
    "        user,\n",
    "        perspective\n",
    "):\n",
    "    \"Get an existing summary, if exists, for a combination of user and perspective.\"\n",
    "\n",
    "    print(\"Trying to get summary\")\n",
    "    print(f\"summary key: {user}-{perspective}\")\n",
    "\n",
    "    try:\n",
    "        response = meta_kb_table.get_item(\n",
    "            Key={\n",
    "                \"summary_key\": f\"{user}-{perspective}\",\n",
    "            }\n",
    "        )\n",
    "        if \"Item\" in response:\n",
    "            item = response[\"Item\"]\n",
    "            return item[\"summary\"]\n",
    "        else:\n",
    "            return \"\"\n",
    "    except ClientError as ex:\n",
    "        print(f\"Summary for {user}-{perspective} does not exist\")\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b227660-5259-45f9-80e0-d24eade35bab",
   "metadata": {},
   "source": [
    "## Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ac778-04e6-48a6-80c7-b22fbafc891c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T04:13:57.863995Z",
     "iopub.status.busy": "2025-06-23T04:13:57.863587Z",
     "iopub.status.idle": "2025-06-23T04:13:57.867887Z",
     "shell.execute_reply": "2025-06-23T04:13:57.867098Z",
     "shell.execute_reply.started": "2025-06-23T04:13:57.863966Z"
    }
   },
   "outputs": [],
   "source": [
    "document_text = extract_text_from_pdf(PDF_FILE)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33181a-c544-4b2c-9a33-29e5aaea9a15",
   "metadata": {},
   "source": [
    "## Metadata generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df853e7e",
   "metadata": {},
   "source": [
    "In this section we will extract metadata from the document and store it in a Pydantic **DocumentMetadata** object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806cadef-7077-4b6d-88b6-7915274bf13a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T04:13:57.881744Z",
     "iopub.status.busy": "2025-06-23T04:13:57.881426Z",
     "iopub.status.idle": "2025-06-23T04:13:57.976999Z",
     "shell.execute_reply": "2025-06-23T04:13:57.975972Z",
     "shell.execute_reply.started": "2025-06-23T04:13:57.881716Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_llm = ChatBedrockConverse(\n",
    "    model=BEDROCK_MODEL_ID,\n",
    "    temperature=0.4,\n",
    "    max_tokens=2000,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "LLM_GENERATE_METADATA_PROMPT_SELECTOR = get_metadata_prompt_selector(lang=\"en\")\n",
    "\n",
    "gen_medatata_prompt = LLM_GENERATE_METADATA_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "structured_metadata = metadata_llm.with_structured_output(DocumentMetadata)\n",
    "\n",
    "structured_metadata_prompt = gen_medatata_prompt | structured_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88eb48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_metadata = structured_metadata_prompt.invoke(\n",
    "    {\n",
    "        \"document_types\": \", \".join([e.value for e in DocumentTypes]),\n",
    "        \"user_type\": ANALYSIS_PERSONNA,\n",
    "        \"doc_title\": DOCUMENT_TITLE,\n",
    "        \"text\": document_text,\n",
    "        \"medata_object_definition\": DocumentMetadata.model_json_schema()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb865c2",
   "metadata": {},
   "source": [
    "Lets take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61788a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated metadata\\n\")\n",
    "\n",
    "for k, v in document_metadata.model_dump().items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8464ac-ca7c-41f3-8634-fb99816491e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T04:52:33.211440Z",
     "iopub.status.busy": "2025-06-11T04:52:33.210829Z",
     "iopub.status.idle": "2025-06-11T04:52:33.215051Z",
     "shell.execute_reply": "2025-06-11T04:52:33.213926Z",
     "shell.execute_reply.started": "2025-06-11T04:52:33.211413Z"
    }
   },
   "source": [
    "## Q&A Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada494f",
   "metadata": {},
   "source": [
    "We now generate pairs of QA from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba56d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_llm = ChatBedrockConverse(\n",
    "    model=BEDROCK_MODEL_ID,\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "LLM_GENERATE_QUESTIONS_PROMPT_SELECTOR = get_qa_prompt_selector(lang=\"en\")\n",
    "\n",
    "gen_questions_prompt = LLM_GENERATE_QUESTIONS_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "\n",
    "questions_prompt = gen_questions_prompt | questions_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5837bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qa_pairs = 10\n",
    "\n",
    "qa_completion = questions_prompt.invoke(\n",
    "    {\n",
    "        \"topic_perspective\": ANALYSIS_PERSPECTIVE,\n",
    "        \"user_type\": ANALYSIS_PERSONNA,\n",
    "        \"n_pairs\": n_qa_pairs,\n",
    "        \"doc_title\": DOCUMENT_TITLE,\n",
    "        \"text\": document_text\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f396dfc",
   "metadata": {},
   "source": [
    "Next, we extract only the question-answers to a Pydantic **QA_pairs** object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b7346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_extraction_llm = ChatBedrockConverse(\n",
    "    model=BEDROCK_MODEL_ID,\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "LLM_GENERATE_STRUCTURED_QUESTIONS_PROMPT_SELECTOR = get_structured_qa_prompt_selector(lang=\"en\")\n",
    "\n",
    "gen_structured_questions_prompt = LLM_GENERATE_STRUCTURED_QUESTIONS_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "structured_questions = questions_llm.with_structured_output(QA_pairs)\n",
    "\n",
    "structured_questions_prompt = gen_structured_questions_prompt | structured_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6baa82-6794-4ef9-8cb8-c7c9a5bca971",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_qa = structured_questions_prompt.invoke(\n",
    "    {\n",
    "        \"qa_text\": qa_completion\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c524a86",
   "metadata": {},
   "source": [
    "Lets visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b35789-7d58-498d-b767-5d51f05950a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in document_qa.qa_pairs:\n",
    "    print(f\"Q: {qa.question}\")\n",
    "    print(f\"A: {qa.answer}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb9490-720f-4f73-9985-21904f103455",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Meta-Knowledge summary generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31c1ed",
   "metadata": {},
   "source": [
    "Now we can take the metadata and question-answer pairs to generate the meta-knowledge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_summary = get_existing_summary(ANALYSIS_PERSONNA, ANALYSIS_PERSPECTIVE)\n",
    "\n",
    "print(\"Existing summary:\\n\\n\")\n",
    "print(previous_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03f444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_qa_str = \"\"\n",
    "\n",
    "for i, qa_pair in enumerate(document_qa.qa_pairs):\n",
    "    document_qa_str += f\"{i+1}.- {qa_pair.question}\\n{qa_pair.answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a8d698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_summary_llm = ChatBedrockConverse(\n",
    "    model=BEDROCK_MODEL_ID,\n",
    "    temperature=0.4,\n",
    "    max_tokens=5000,\n",
    "    top_p=0.9\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c85c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_GENERATE_SUMMARY_PROMPT_SELECTOR = get_summary_prompt_selector(lang=\"en\", with_context=True if previous_summary else False, for_chunks=False)\n",
    "\n",
    "gen_summary_prompt = LLM_GENERATE_SUMMARY_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "\n",
    "summary_prompt = gen_summary_prompt | mk_summary_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e83d2-402c-4488-86dc-2fecd8911f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if previous_summary:\n",
    "\n",
    "    print(\"Generating with previous summary\")\n",
    "    mk_summary = summary_prompt.invoke(\n",
    "        {\n",
    "            \"document_types\": \", \".join([e.value for e in DocumentTypes]),\n",
    "            \"topic_perspective\": ANALYSIS_PERSPECTIVE,\n",
    "            \"users_types\": ANALYSIS_PERSONNA,\n",
    "            \"qa_pairs\": document_qa_str,\n",
    "            \"summary\": previous_summary\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Generating without previous summary\")\n",
    "    mk_summary = summary_prompt.invoke(\n",
    "        {\n",
    "            \"document_types\": \", \".join([e.value for e in DocumentTypes]),\n",
    "            \"topic_perspective\": ANALYSIS_PERSPECTIVE,\n",
    "            \"users_types\": ANALYSIS_PERSONNA,\n",
    "            \"qa_pairs\": document_qa_str\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae4bcb",
   "metadata": {},
   "source": [
    "The generated summary is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637b943-97f9-41df-b886-48e29d1752d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_summary.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e88b3-8cc8-4875-a869-d3d221258897",
   "metadata": {},
   "source": [
    "## Indexing into knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28df2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=qa_pair.question, \n",
    "        metadata={\n",
    "            \"persona\": ANALYSIS_PERSONNA,\n",
    "            \"perspective\": ANALYSIS_PERSPECTIVE,\n",
    "            \"question\": qa_pair.question,\n",
    "            \"answer\": qa_pair.answer,\n",
    "        }\n",
    "    ) for qa_pair in document_qa.qa_pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36fa4e",
   "metadata": {},
   "source": [
    "We are now ready to index the information into the knowledge base. We will be storing the meta-knowledge summary into an Amazon DynamoDB table and the Q&A pairs into an Amazon OpenSearch Serverless vector-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc401ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_kb_table.put_item(\n",
    "    Item={\n",
    "        \"summary_key\": f\"{ANALYSIS_PERSONNA}-{ANALYSIS_PERSPECTIVE}\",\n",
    "        \"summary\": mk_summary.content,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f6197",
   "metadata": {},
   "source": [
    "Embed and store the queston-answer pairs in OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1cfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_client = get_opensearch_connection(OPENSEARCH_HOST, OPENSEARCH_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3665c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa_pair in document_qa.qa_pairs:\n",
    "\n",
    "    question = qa_pair.question\n",
    "    answer = qa_pair.answer\n",
    "\n",
    "    print(f\"Q&A pair:\\n\\n{question}\\n{answer}\")\n",
    "    question_vector = encode_text(question)\n",
    "\n",
    "    #Index Q&A pair\n",
    "    oss_response = index_document(\n",
    "        oss_client=opensearch_client,\n",
    "        oss_index_name=OPERNSEARCH_INDEX_NAME,\n",
    "        embedding=question_vector,\n",
    "        persona=ANALYSIS_PERSONNA,\n",
    "        perspective=ANALYSIS_PERSPECTIVE,\n",
    "        question=question,\n",
    "        answer=answer\n",
    "    )\n",
    "\n",
    "    print(f\"Q&A pair indexed: {oss_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a64b23",
   "metadata": {},
   "source": [
    "If you made it to this point now you have a knowledge base with the **prepare-then-rewrite-then-retrieve-then-read** framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c07dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
