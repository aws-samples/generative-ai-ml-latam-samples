{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare-Rewrite-Retrieve RAG flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the data retrieval and generation process of the **prepare-then-rewrite-then-retrieve-then-read** framework proposed by the authors of [\"Meta Knowledge for Retrieval Augmented Large Language Models\"](https://www.amazon.science/publications/meta-knowledge-for-retrieval-augmented-large-language-models) for creating more accurate and enriched RAG workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "To run this notebook your role executing the notebook needs:\n",
    "\n",
    "* Permissions to invoke Bedrock\n",
    "* Access to the Amazon Nova Pro model\n",
    "* Write permissions to the DynamoDB table created with the CDK stack in this PoC\n",
    "* Write permissions to the OpenSearch Serverless host created with the CDK stack in this PoC\n",
    "* Having executed the [DataIndexingWithInfra.ipynb](./DataIndexingWithInfra.ipynb) notebook\n",
    "\n",
    "Additionally, we need the following python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U boto3 langchain langchain-aws opensearch-py dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import secrets\n",
    "import time\n",
    "import boto3\n",
    "import langchain_core\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from enum import Enum\n",
    "from PyPDF2 import PdfReader\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from prompts.dataRetrieval.generate_query_augmentation_prompts import get_query_augmentation_prompt_selector, get_structured_questions_prompt_selector\n",
    "from prompts.dataRetrieval.generate_qa_kb_prompts import get_kb_qa_prompt_selector\n",
    "from structured_output.answers import Answer\n",
    "from structured_output.questions import Questions\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "langchain_core.globals.set_debug(False)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize according to the types of document to be processed by the application\n",
    "class DocumentTypes(Enum):\n",
    "    SYSTEM_ARCHITECTURE = \"systems architecture\"\n",
    "    SECURITY = \"information technology security\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    TECH_STRATEGY = \"tech strategy\"\n",
    "    MANAGEMENT = \"management\"\n",
    "\n",
    "class AnalysisPerspectives(Enum):\n",
    "    SECURITY = \"software security engineer\"\n",
    "    DATA_GOVERNANCE = \"data governance\"\n",
    "    RESILIENCY = \"systems resiliency\"\n",
    "    SYS_OPS = \"systems operations\"\n",
    "\n",
    "# Persona definition for generating and answering QA\n",
    "AnalysisPersonas = {\n",
    "    \"software security engineer\": {\n",
    "        \"description\": \"It is responsible for ensuring that workloads have the necessary security controls in place\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SECURITY.value, AnalysisPerspectives.DATA_GOVERNANCE.value]\n",
    "    },\n",
    "    \"solutions architect\": {\n",
    "        \"description\": \"It is responsible for designing scalable and cost-efficient software solutions\",\n",
    "        \"perspectives\": [AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.DATA_GOVERNANCE.value, AnalysisPerspectives.SECURITY.value]\n",
    "    },\n",
    "    \"software developer\": {\n",
    "        \"description\":\"Implements the system functionalities\",\n",
    "        \"perspectives\": [AnalysisPerspectives.SYS_OPS.value, AnalysisPerspectives.RESILIENCY.value, AnalysisPerspectives.SECURITY.value]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "EMBEDDINGS_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "OPENSEARCH_HOST = <OPENSEARCH_HOST> #URL (without the protocol) of the OpenSearch Serverless Host\n",
    "OPENSEARCH_PORT = 443 #Port of the OpenSearch Serverless Host\n",
    "OPERNSEARCH_INDEX_NAME = <OPENSEARCH_INDEX_NAME> #Name of the OpenSearch Serverless Index\n",
    "\n",
    "METAKB_DYNAMODB_TABLE_NAME = <METAKB_DYNAMODB_TABLE_NAME> #Name of the DynamoDB table created with the CDK stack in this PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_PERSONNA = \"solutions architect\"\n",
    "ANALYSIS_PERSPECTIVE = AnalysisPersonas[ANALYSIS_PERSONNA][\"perspectives\"][0]\n",
    "\n",
    "print(f\"Using persona: {ANALYSIS_PERSONNA}\")\n",
    "print(f\"Using perspective: {ANALYSIS_PERSPECTIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clients for AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "botoSession = boto3.Session()\n",
    "\n",
    "meta_kb_table = botoSession.resource(\"dynamodb\").Table(METAKB_DYNAMODB_TABLE_NAME)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(\n",
    "        text: str = None,  # the text to be encoded\n",
    "        dimension: int = 1024,  # 1,024 (default), 384, 256\n",
    "):\n",
    "    \"Get text embedding using embeddings model\"\n",
    "\n",
    "    payload_body = {\n",
    "        \"inputText\": text,\n",
    "        \"dimensions\": dimension,\n",
    "        \"normalize\": True\n",
    "    }\n",
    "\n",
    "    #print(\"embedding text\")\n",
    "    #print(payload_body)\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload_body),\n",
    "        modelId=EMBEDDINGS_MODEL_ID,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    feature_vector = json.loads(response.get(\"body\").read())[\"embedding\"]\n",
    "\n",
    "    #print(\"text embedding\")\n",
    "    #print(feature_vector)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def qa_chatbot_answer(\n",
    "    user_query,\n",
    "    role,\n",
    "    perspective,\n",
    "    context\n",
    "):\n",
    "    \"Answer query given context using LLMs\"\n",
    "\n",
    "    rag_llm = ChatBedrockConverse(\n",
    "        model=BEDROCK_MODEL_ID,\n",
    "        temperature=0.4,\n",
    "        max_tokens=1000,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    LLM_KB_QA_PROMPT_SELECTOR = get_kb_qa_prompt_selector(lang=\"en\")\n",
    "    \n",
    "    gen_kb_qa_prompt = LLM_KB_QA_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "    \n",
    "    kb_qa_generate = gen_kb_qa_prompt | rag_llm.with_structured_output(Answer)\n",
    "\n",
    "    rag_qa = kb_qa_generate.invoke(\n",
    "        {\n",
    "            \"question\": user_query,\n",
    "            \"role\": role,\n",
    "            \"perspective\": perspective,\n",
    "            \"context\": context\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return rag_qa\n",
    "\n",
    "def get_opensearch_connection(\n",
    "        host: str,\n",
    "        port: int,\n",
    ") -> OpenSearch:\n",
    "    \"Establishes a connection to an OpenSearch cluster using AWSV4SignerAuth for authentication.\"\n",
    "\n",
    "    # Create an AWSV4SignerAuth instance for authentication\n",
    "    auth = AWSV4SignerAuth(\n",
    "        boto3.Session(\n",
    "            region_name=os.getenv(\"AWS_REGION\")\n",
    "        ).get_credentials(),\n",
    "        os.getenv(\"AWS_REGION\"),\n",
    "        \"aoss\"\n",
    "    )\n",
    "\n",
    "    # Create an OpenSearch client instance\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": port}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=30,\n",
    "    )\n",
    "\n",
    "    # Return the OpenSearch client instance\n",
    "    return client\n",
    "\n",
    "def get_existing_summary(\n",
    "        user,\n",
    "        perspective\n",
    "):\n",
    "    \"Get an existing summary, if exists, for a combination of user and perspective.\"\n",
    "\n",
    "    print(\"Trying to get summary\")\n",
    "    print(f\"summary key: {user}-{perspective}\")\n",
    "\n",
    "    try:\n",
    "        response = meta_kb_table.get_item(\n",
    "            Key={\n",
    "                \"summary_key\": f\"{user}-{perspective}\",\n",
    "            }\n",
    "        )\n",
    "        if \"Item\" in response:\n",
    "            item = response[\"Item\"]\n",
    "            return item[\"summary\"]\n",
    "        else:\n",
    "            return \"\"\n",
    "    except ClientError as ex:\n",
    "        print(f\"Summary for {user}-{perspective} does not exist\")\n",
    "        raise ex\n",
    "\n",
    "def search_knowledge_base(\n",
    "    index_name,\n",
    "    embedding,\n",
    "    oss_client,\n",
    "    persona,\n",
    "    perspective,\n",
    "    k=3\n",
    "):\n",
    "    \"Given the parameter retrieve context from the KB\"\n",
    "\n",
    "    print(f\"Looking for data for {persona} and {perspective}\")\n",
    "    \n",
    "    matched_qa_pairs = []\n",
    "\n",
    "    body = {\n",
    "        \"size\": k,\n",
    "        \"_source\": {\n",
    "            \"exclude\": [\"embedding\"],\n",
    "        },\n",
    "        \"query\":\n",
    "            {\n",
    "                \"knn\":\n",
    "                    {\n",
    "                        \"embedding\": {\n",
    "                            \"vector\": embedding,\n",
    "                            \"k\": k,\n",
    "                        }\n",
    "                    }\n",
    "            },\n",
    "        \"post_filter\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"term\": {\"persona\": persona}},\n",
    "                    {\"term\": {\"perspective\": perspective}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = oss_client.search(index=index_name, body=body)\n",
    "\n",
    "    print(\"The results\")\n",
    "    print(res)\n",
    "\n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        matched_qa_pairs.append((hit[\"_source\"][\"question\"], hit[\"_source\"][\"answer\"]))\n",
    "\n",
    "    return matched_qa_pairs\n",
    "\n",
    "def augment_user_query(\n",
    "        role,\n",
    "        user_query,\n",
    "        mk_summary,\n",
    "):\n",
    "    \"Augment the user query with additional queries based on the meta-knowledge summary.\"\n",
    "    \n",
    "    query_augmentation_llm = ChatBedrockConverse(\n",
    "        model=BEDROCK_MODEL_ID,\n",
    "        temperature=0.4,\n",
    "        max_tokens=2000,\n",
    "        # other params...\n",
    "    )\n",
    "    \n",
    "    LLM_AUGMENT_QUERY_PROMPT_SELECTOR = get_query_augmentation_prompt_selector(lang=\"en\")\n",
    "    \n",
    "    gen_queries_prompt = LLM_AUGMENT_QUERY_PROMPT_SELECTOR.get_prompt(BEDROCK_MODEL_ID)\n",
    "    structured_queries = query_augmentation_llm.with_structured_output(Questions)\n",
    "    \n",
    "    structured_queries_generate = gen_queries_prompt | structured_queries\n",
    "\n",
    "    augmented_queries = structured_queries_generate.invoke(\n",
    "        {\n",
    "            \"role\": role,\n",
    "            \"mk_summary\": mk_summary,\n",
    "            \"user_query\": user_query\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return augmented_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering workflow with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is the purpose of the multi-agent compliance analysis project?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the original query is augmented using the meta-knowledge summary for the persona-perspective combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_kb_summary = get_existing_summary(ANALYSIS_PERSONNA, ANALYSIS_PERSPECTIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_queries = augment_user_query(\n",
    "    role=ANALYSIS_PERSONNA,\n",
    "    user_query=QUERY,\n",
    "    mk_summary=meta_kb_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmenting the query:\")\n",
    "print(QUERY)\n",
    "\n",
    "print(\"\\n\\nExisting summary:\")\n",
    "print(meta_kb_summary)\n",
    "\n",
    "print(\"\\n\\nResulting queries:\")\n",
    "for question in augmented_queries.questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the augmented queries and retrieve information from the knowledge base using the augmented queries rather than the original query wich we will pass onto the LLM as context for the question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_client = get_opensearch_connection(OPENSEARCH_HOST, OPENSEARCH_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_str = \"\"\n",
    "qa_pairs = []\n",
    "\n",
    "for query in augmented_queries.questions:\n",
    "    \n",
    "    embedding = encode_text(text=query)\n",
    "    retrieved_qa_pairs = search_knowledge_base(\n",
    "        oss_client=opensearch_client,\n",
    "        index_name=OPERNSEARCH_INDEX_NAME,\n",
    "        embedding=embedding,\n",
    "        persona=ANALYSIS_PERSONNA,\n",
    "        perspective=ANALYSIS_PERSPECTIVE,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    qa_pairs.extend(retrieved_qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can observe that we obtain indeed more comprehensive information but more fine-grained thanks to the indexing of Q&A pairs rather than chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa_pair in qa_pairs:\n",
    "    print(f\"Question: {qa_pair[0]}\")\n",
    "    print(f\"Answer: {qa_pair[1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering the original query with the context from augmented queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the retrieved information from the augmented queries as context to answer the original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_str = qa_str.join(f\"Question:{qa[0]}\\nAnswer:{qa[1]}\\n\\n\" for qa in qa_pairs)\n",
    "\n",
    "answer = qa_chatbot_answer(\n",
    "    user_query=QUERY,\n",
    "    role=ANALYSIS_PERSONNA,\n",
    "    perspective=ANALYSIS_PERSPECTIVE,\n",
    "    context=qa_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QUERY)\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
